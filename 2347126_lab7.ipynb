{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 'PoetryFoundationData.csv' to 'output.txt'\n"
     ]
    }
   ],
   "source": [
    "# prompt: code to convert csv file to txt file and remove all the extra \\t and \\n in that file\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def convert_csv_to_txt(csv_filepath, txt_filepath):\n",
    "    \"\"\"Converts a CSV file to a TXT file, removing extra tabs and newlines.\"\"\"\n",
    "    try:\n",
    "        with open(csv_filepath, 'r', encoding='utf-8') as csvfile, open(txt_filepath, 'w', encoding='utf-8') as txtfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                cleaned_row = [re.sub(r'[\\t\\n]+', '', item) for item in row]  # Remove tabs and newlines from each item\n",
    "                txtfile.write(','.join(cleaned_row) + '\\n')  # Join items back with commas and write to txt file\n",
    "        print(f\"Successfully converted '{csv_filepath}' to '{txt_filepath}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_filepath}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage: Replace 'input.csv' and 'output.txt' with your filepaths.\n",
    "convert_csv_to_txt('PoetryFoundationData.csv', 'output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mku\u001b[39;00m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.keras.utils as ku \n",
    "from wordcloud import WordCloud \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Reading the text data file \n",
    "data = open('output.txt', encoding=\"utf8\").read() \n",
    "\n",
    "# EDA: Generating WordCloud to visualize \n",
    "# the text \n",
    "wordcloud = WordCloud(max_font_size=50, \n",
    "\t\t\t\t\tmax_words=100, \n",
    "\t\t\t\t\tbackground_color=\"black\").generate(data) \n",
    "\n",
    "# Plotting the WordCloud \n",
    "plt.figure(figsize=(8, 4)) \n",
    "plt.imshow(wordcloud, interpolation='bilinear') \n",
    "plt.axis(\"off\") \n",
    "plt.savefig(\"WordCloud.png\") \n",
    "plt.show() \n",
    "\n",
    "\n",
    "# Generating the corpus by \n",
    "# splitting the text into lines \n",
    "corpus = data.lower().split(\"\\n\") \n",
    "print(corpus[:10]) \n",
    "\n",
    "\n",
    "# Fitting the Tokenizer on the Corpus \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(corpus) \n",
    "\n",
    "# Vocabulary count of the corpus \n",
    "total_words = len(tokenizer.word_index) \n",
    "\n",
    "print(\"Total Words:\", total_words) \n",
    "\n",
    "\n",
    "# Converting the text into embeddings \n",
    "input_sequences = [] \n",
    "for line in corpus: \n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] \n",
    "\n",
    "\tfor i in range(1, len(token_list)): \n",
    "\t\tn_gram_sequence = token_list[:i+1] \n",
    "\t\tinput_sequences.append(n_gram_sequence) \n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences(input_sequences, \n",
    "\t\t\t\t\t\t\t\t\t\tmaxlen=max_sequence_len, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding='pre')) \n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "label = ku.to_categorical(label, num_classes=total_words+1) \n",
    "\n",
    "\n",
    "# Building a Bi-Directional LSTM Model \n",
    "model = Sequential() \n",
    "model.add(Embedding(total_words+1, 100, \n",
    "\t\t\t\t\tinput_length=max_sequence_len-1)) \n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True))) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(total_words+1/2, activation='relu', \n",
    "\t\t\t\tkernel_regularizer=regularizers.l2(0.01))) \n",
    "model.add(Dense(total_words+1, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "\t\t\toptimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary()) \n",
    "\n",
    "\n",
    "history = model.fit(predictors, label, epochs=150, verbose=1)\n",
    "\n",
    "\n",
    "seed_text = \"a short phrase\"\n",
    "next_words = 25\n",
    "ouptut_text = \"\" \n",
    "\n",
    "for _ in range(next_words): \n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "\ttoken_list = pad_sequences( \n",
    "\t\t[token_list], maxlen=max_sequence_len-1, \n",
    "\tpadding='pre') \n",
    "\tpredicted = np.argmax(model.predict(token_list, \n",
    "\t\t\t\t\t\t\t\t\t\tverbose=0), axis=-1) \n",
    "\toutput_word = \"\" \n",
    "\t\n",
    "\tfor word, index in tokenizer.word_index.items(): \n",
    "\t\tif index == predicted: \n",
    "\t\t\toutput_word = word \n",
    "\t\t\tbreak\n",
    "\t\t\t\n",
    "\tseed_text += \" \" + output_word \n",
    "\t\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
