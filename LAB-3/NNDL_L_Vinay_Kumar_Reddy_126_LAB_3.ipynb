{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Data Preprocessing:#\n",
        "\n",
        "*Load the CIFAR-10 dataset.*\n",
        "\n",
        "*Perform necessary data preprocessing steps:*\n",
        "\n",
        "  ▪ Normalize pixel values to range between 0 and 1.\n",
        "\n",
        "  ▪ Convert class labels into one-hot encoded format.\n",
        "\n",
        "  ▪ Split the dataset into training and test sets (e.g., 50,000 images for training and 10,000 for testing).\n",
        "\n",
        "  ▪ Optionally, apply data augmentation techniques (such as random flips, rotations, or shifts) to improve the generalization of the model."
      ],
      "metadata": {
        "id": "D42aWNu8IktL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxQdCEp9gP3",
        "outputId": "4df5770e-ee92-4429-9ee3-69b3eb04990b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape (50000, 3072)\n",
            "Training labels shape (50000, 10)\n",
            "Testing data shape (10000, 3072)\n",
            "Testing labels shape (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert class labels to one-hot encoded format\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Flatten the input data for ANN (from 32x32x3 to 3072)\n",
        "x_train = x_train.reshape(-1, 32*32*3)\n",
        "x_test = x_test.reshape(-1, 32*32*3)\n",
        "\n",
        "print(\"Training data shape\", x_train.shape)\n",
        "print(\"Training labels shape\", y_train.shape)\n",
        "print(\"Testing data shape\", x_test.shape)\n",
        "print(\"Testing labels shape\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**: Scale pixel values to a range between 0 and 1.\n",
        "\n",
        "**One-Hot Encoding**: Convert the class labels into one-hot encoded format for multi-class classification.\n",
        "\n",
        "**Data Splitting**: Split the dataset into training (50,000) and testing (10,000) images."
      ],
      "metadata": {
        "id": "hOPmKUgtKnv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Network Architecture Design:#\n",
        "\n",
        "Design a feedforward neural network to classify the images.\n",
        "\n",
        "*▪ Input Layer*: The input shape should match the 32x32x3 dimensions of the CIFAR-10 images.\n",
        "\n",
        "*▪ Hidden Layers*: Use appropriate layers.\n",
        "\n",
        "*▪ Output Layer*: The final layer should have 10 output neurons (one for each class) with a softmax activation function for multi-class classification."
      ],
      "metadata": {
        "id": "9_CWDRyjKb3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define the ANN model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer (3072 features from the 32x32x3 image)\n",
        "model.add(Dense(512, activation='relu', input_shape=(32*32*3,)))\n",
        "\n",
        "# Hidden layers with ReLU and Dropout for regularization\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Dropout to avoid overfitting\n",
        "\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer (10 classes with softmax for multi-class classification)\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "nImeh_f--JkU",
        "outputId": "73e173ab-fdd7-4f1c-8ae5-8bf47f990086"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,573,376\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,376</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,738,890\u001b[0m (6.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,738,890</span> (6.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,738,890\u001b[0m (6.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,738,890</span> (6.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Input Layer*: 32x32x3 (RGB image).\n",
        "\n",
        "*Convolutional Layers*: To detect patterns like edges, colors, or textures.\n",
        "\n",
        "*Pooling Layers*: To downsample the image and reduce complexity.\n",
        "\n",
        "*Fully Connected Layers*: To classify the extracted features into categories.\n",
        "\n",
        "*Output Layer*: 10 neurons with softmax activation for multi-class classification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Justification**\n",
        "\n",
        "*Convolutional layers* help in automatically learning filters for feature extraction.\n",
        "\n",
        "*Pooling layers* reduce the number of parameters and computational load.\n",
        "\n",
        "*Fully connected layers* consolidate the extracted features into final class scores."
      ],
      "metadata": {
        "id": "1ajbR8suMyce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Activation Functions\n",
        "\n",
        "*ReLU* (Rectified Linear Unit) is efficient for preventing the vanishing gradient problem during backpropagation by allowing faster learning.\n",
        "\n",
        "*tanh* ensures that the values are centered around zero, which can improve convergence in some cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "PsUMqik1NtBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No change needed in the previous code as ReLU is already used."
      ],
      "metadata": {
        "id": "PHA1aVz5N5wB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role in Backpropagation:**\n",
        "\n",
        "*ReLU*: ReLU mitigates the vanishing gradient problem (which is common with Sigmoid and Tanh) because its gradient does not saturate (except for the zero output case).ReLU deactivates neurons when the input is negative (output is 0), making the model sparse and more computationally efficient.\n",
        "\n",
        "*tanh*: can be useful in cases where the input data is centered around zero, but it may suffer from the vanishing gradient problem in deeper layers."
      ],
      "metadata": {
        "id": "1gylZ9ChN-C1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Loss Function and Optimizer\n",
        "\n",
        "The most suitable loss function for multi-class classification is categorical crossentropy. You could compare this with:\n",
        "\n",
        "  *Mean Squared Error (MSE)*: Not ideal for classification but used to compare performance.\n",
        "  \n",
        "  *Sparse Categorical Crossentropy*: Another variant of cross-entropy when the labels are integers.\n",
        "\n",
        "Use Adam optimizer due to its adaptive learning rate and ability to handle sparse gradients."
      ],
      "metadata": {
        "id": "0EJK3xFSOIqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "QIyDfNsr-e7m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Effect of Optimizer & Learning Rate:*\n",
        "\n",
        "Adam adjusts the learning rate dynamically, leading to faster convergence.\n",
        "  \n",
        "If the model isn't converging, reduce the learning rate to allow for finer updates."
      ],
      "metadata": {
        "id": "Cvu21cfyO9u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Training the Model:\n",
        "\n",
        "Implement backpropagation to update the weights and biases of the\n",
        "network during training.\n",
        "\n",
        "Train the model for a fixed number of epochs (e.g., 50 epochs) and\n",
        "monitor the training and validation accuracy."
      ],
      "metadata": {
        "id": "qM0ydBozQ09x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (batch size of 64 and 50 epochs)\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey73RJnz-hel",
        "outputId": "3a301aae-65ef-4181-ff24-245e29fc718c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.1552 - loss: 2.6320 - val_accuracy: 0.3099 - val_loss: 1.8767\n",
            "Epoch 2/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3168 - loss: 1.8655 - val_accuracy: 0.3550 - val_loss: 1.7682\n",
            "Epoch 3/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3337 - loss: 1.8257 - val_accuracy: 0.3673 - val_loss: 1.7387\n",
            "Epoch 4/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3409 - loss: 1.7992 - val_accuracy: 0.3733 - val_loss: 1.7248\n",
            "Epoch 5/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3614 - loss: 1.7763 - val_accuracy: 0.3783 - val_loss: 1.7143\n",
            "Epoch 6/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3620 - loss: 1.7554 - val_accuracy: 0.3723 - val_loss: 1.7151\n",
            "Epoch 7/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3685 - loss: 1.7403 - val_accuracy: 0.3816 - val_loss: 1.6966\n",
            "Epoch 8/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3773 - loss: 1.7181 - val_accuracy: 0.4048 - val_loss: 1.6604\n",
            "Epoch 9/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3838 - loss: 1.7164 - val_accuracy: 0.4040 - val_loss: 1.6692\n",
            "Epoch 10/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3877 - loss: 1.6961 - val_accuracy: 0.4022 - val_loss: 1.6640\n",
            "Epoch 11/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3951 - loss: 1.6878 - val_accuracy: 0.3919 - val_loss: 1.6895\n",
            "Epoch 12/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3949 - loss: 1.6804 - val_accuracy: 0.4141 - val_loss: 1.6362\n",
            "Epoch 13/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4059 - loss: 1.6702 - val_accuracy: 0.4128 - val_loss: 1.6388\n",
            "Epoch 14/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4036 - loss: 1.6684 - val_accuracy: 0.4139 - val_loss: 1.6243\n",
            "Epoch 15/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4089 - loss: 1.6511 - val_accuracy: 0.4078 - val_loss: 1.6748\n",
            "Epoch 16/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4052 - loss: 1.6561 - val_accuracy: 0.4153 - val_loss: 1.6438\n",
            "Epoch 17/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4157 - loss: 1.6421 - val_accuracy: 0.4179 - val_loss: 1.6238\n",
            "Epoch 18/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4130 - loss: 1.6383 - val_accuracy: 0.4208 - val_loss: 1.6243\n",
            "Epoch 19/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4124 - loss: 1.6362 - val_accuracy: 0.4262 - val_loss: 1.6074\n",
            "Epoch 20/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4175 - loss: 1.6288 - val_accuracy: 0.4240 - val_loss: 1.6126\n",
            "Epoch 21/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4145 - loss: 1.6290 - val_accuracy: 0.4254 - val_loss: 1.5939\n",
            "Epoch 22/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4208 - loss: 1.6191 - val_accuracy: 0.4254 - val_loss: 1.6119\n",
            "Epoch 23/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4202 - loss: 1.6188 - val_accuracy: 0.4384 - val_loss: 1.5773\n",
            "Epoch 24/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4231 - loss: 1.6113 - val_accuracy: 0.4421 - val_loss: 1.5734\n",
            "Epoch 25/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4260 - loss: 1.6054 - val_accuracy: 0.4206 - val_loss: 1.6191\n",
            "Epoch 26/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4204 - loss: 1.6215 - val_accuracy: 0.4220 - val_loss: 1.6222\n",
            "Epoch 27/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4280 - loss: 1.6083 - val_accuracy: 0.4305 - val_loss: 1.5984\n",
            "Epoch 28/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4291 - loss: 1.5953 - val_accuracy: 0.4320 - val_loss: 1.5823\n",
            "Epoch 29/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4258 - loss: 1.6027 - val_accuracy: 0.4364 - val_loss: 1.5797\n",
            "Epoch 30/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4298 - loss: 1.5961 - val_accuracy: 0.4288 - val_loss: 1.5952\n",
            "Epoch 31/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4295 - loss: 1.5993 - val_accuracy: 0.4325 - val_loss: 1.6086\n",
            "Epoch 32/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4334 - loss: 1.5843 - val_accuracy: 0.4437 - val_loss: 1.5594\n",
            "Epoch 33/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4351 - loss: 1.5852 - val_accuracy: 0.4344 - val_loss: 1.5834\n",
            "Epoch 34/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4342 - loss: 1.5794 - val_accuracy: 0.4422 - val_loss: 1.5607\n",
            "Epoch 35/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4366 - loss: 1.5766 - val_accuracy: 0.4275 - val_loss: 1.5992\n",
            "Epoch 36/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4330 - loss: 1.5863 - val_accuracy: 0.4488 - val_loss: 1.5587\n",
            "Epoch 37/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4375 - loss: 1.5689 - val_accuracy: 0.4350 - val_loss: 1.6005\n",
            "Epoch 38/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4388 - loss: 1.5768 - val_accuracy: 0.4476 - val_loss: 1.5720\n",
            "Epoch 39/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4368 - loss: 1.5705 - val_accuracy: 0.4393 - val_loss: 1.5753\n",
            "Epoch 40/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4363 - loss: 1.5720 - val_accuracy: 0.4476 - val_loss: 1.5504\n",
            "Epoch 41/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4373 - loss: 1.5691 - val_accuracy: 0.4114 - val_loss: 1.6363\n",
            "Epoch 42/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4417 - loss: 1.5624 - val_accuracy: 0.4354 - val_loss: 1.5883\n",
            "Epoch 43/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4459 - loss: 1.5598 - val_accuracy: 0.4395 - val_loss: 1.5676\n",
            "Epoch 44/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4401 - loss: 1.5594 - val_accuracy: 0.4401 - val_loss: 1.5761\n",
            "Epoch 45/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4470 - loss: 1.5567 - val_accuracy: 0.4349 - val_loss: 1.5849\n",
            "Epoch 46/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4470 - loss: 1.5523 - val_accuracy: 0.4449 - val_loss: 1.5622\n",
            "Epoch 47/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4447 - loss: 1.5530 - val_accuracy: 0.4425 - val_loss: 1.5735\n",
            "Epoch 48/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4484 - loss: 1.5519 - val_accuracy: 0.4499 - val_loss: 1.5479\n",
            "Epoch 49/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4513 - loss: 1.5453 - val_accuracy: 0.4449 - val_loss: 1.5692\n",
            "Epoch 50/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4503 - loss: 1.5412 - val_accuracy: 0.4500 - val_loss: 1.5541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation & Learning Rate:\n",
        "\n",
        "Backpropagation updates the weights in each layer by calculating the gradient of the loss with respect to the weights and adjusting them using the learning rate.\n",
        "\n",
        "The learning rate determines how large these weight updates are. If it's too high, the model may overshoot optimal points; if too low, it might converge slowly."
      ],
      "metadata": {
        "id": "mSJOFZ1AQcwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Model Evaluation:\n",
        "After training, evaluate the performance of your model on the test set.\n",
        "\n",
        "Calculate accuracy, precision, recall, F1-score, and the confusion matrix to understand the model’s classification performance."
      ],
      "metadata": {
        "id": "Tko20RCzQyM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Get predictions\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_true = y_test.argmax(axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# Confusion matrix\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "rvt7H8EB-jYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f82b5d-1040-4d18-f4f8-57639b83d6ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4525 - loss: 1.5387\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.55      0.50      1000\n",
            "           1       0.58      0.56      0.57      1000\n",
            "           2       0.36      0.17      0.23      1000\n",
            "           3       0.30      0.35      0.32      1000\n",
            "           4       0.42      0.32      0.36      1000\n",
            "           5       0.41      0.32      0.36      1000\n",
            "           6       0.48      0.52      0.50      1000\n",
            "           7       0.43      0.60      0.50      1000\n",
            "           8       0.50      0.67      0.57      1000\n",
            "           9       0.52      0.46      0.49      1000\n",
            "\n",
            "    accuracy                           0.45     10000\n",
            "   macro avg       0.45      0.45      0.44     10000\n",
            "weighted avg       0.45      0.45      0.44     10000\n",
            "\n",
            "[[549  32  28  34  19  11  14  64 204  45]\n",
            " [ 68 559  11  42  14  12  13  31 110 140]\n",
            " [142  32 166 102 148  76 138 142  36  18]\n",
            " [ 46  28  46 350  55 164 106 118  46  41]\n",
            " [ 80  14  84  89 318  52 144 173  35  11]\n",
            " [ 40  20  56 212  45 317  86 145  53  26]\n",
            " [ 16  11  31 188  86  49 515  57  22  25]\n",
            " [ 70  23  21  58  58  70  24 596  27  53]\n",
            " [144  45   9  33   7  10   8  15 668  61]\n",
            " [ 55 192   5  47   2  17  15  59 146 462]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*How to Improve Performance:*\n",
        "\n",
        " Data Augmentation: Introduce variations in the data to reduce overfitting.\n",
        "\n",
        " More Complex Architectures: Add more layers or filters to improve feature extraction."
      ],
      "metadata": {
        "id": "mU0v1uujRJf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Optimization Strategies\n",
        "**Early Stopping**: Stop training when validation accuracy no longer improves.\n",
        "\n",
        "**Learning Rate Scheduling**: Gradually decrease the learning rate to allow finer convergence.\n",
        "\n",
        "**Weight Initialization**: Start with weights near zero, but not zero, to ensure symmetry breaking and efficient learning.\n",
        "\n",
        "**Weight Initialization Importance**:\n",
        "\n",
        "  Poor initialization can cause vanishing/exploding gradients.\n",
        "\n",
        "  Techniques like He initialization for ReLU layers can help achieve faster convergence."
      ],
      "metadata": {
        "id": "KMOrqX0lRPvE"
      }
    }
  ]
}